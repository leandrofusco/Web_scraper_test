{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to web scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # para fazer requisição a pagina\n",
    "from bs4 import BeautifulSoup # pra fazer o parser do código html\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'info_nome_marca' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bd98144d037c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-bd98144d037c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproducts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_detail_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;31m#         write_csv(data,link)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-bd98144d037c>\u001b[0m in \u001b[0;36mget_detail_data\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m     29\u001b[0m     data = {\n\u001b[0;32m     30\u001b[0m         \u001b[1;34m'info'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minfo_nome\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;34m'marca'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minfo_nome_marca\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;34m'price'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     }\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'info_nome_marca' referenced before assignment"
     ]
    }
   ],
   "source": [
    "searchterm = 'celular'\n",
    "\n",
    "# function to get data\n",
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('server responded:', response.status_code)\n",
    "    else:    \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "    return soup\n",
    "\n",
    "# function to get several details data\n",
    "def get_detail_data(soup):\n",
    "    try:\n",
    "        info_nome = soup.find('h1', class_='ui-pdp-title').text\n",
    "    except:\n",
    "        info_nome = ''\n",
    "    try:\n",
    "        price = soup.find('span', class_='price-tag-fraction').text  \n",
    "    except:            \n",
    "        price = ''\n",
    "    try:\n",
    "        info_nome_marca = re.findall(r'^\\w+', info_nome, re.M|re.I)\n",
    "    except:\n",
    "        info_nome_marca = ''\n",
    "\n",
    "    \n",
    "    data = {\n",
    "        'info': info_nome,\n",
    "        'marca':info_nome_marca,\n",
    "        'price': price\n",
    "    }\n",
    "    print(data)\n",
    "\n",
    "# function to get links\n",
    "def get_index_data(soup):\n",
    "    try:\n",
    "        links = soup.find_all('a', class_='ui-search-item__group__element ui-search-link')\n",
    "    except:\n",
    "        links = []\n",
    "    \n",
    "    urls = [item.get('href') for item in links]\n",
    "       \n",
    "    return urls   \n",
    "\n",
    "\n",
    " # Main function   \n",
    "def main():\n",
    "    url = 'https://lista.mercadolivre.com.br/celular#D[A:{searchterm}]'\n",
    "    \n",
    "    products = get_index_data(get_page(url))\n",
    "    \n",
    "    for link in products:\n",
    "        data = get_detail_data(get_page(link))\n",
    "#         write_csv(data,link)\n",
    "        \n",
    "        \n",
    "# # function to save data    \n",
    "# def write_csv(data, url):\n",
    "#     with open('output.csv', 'a') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "        \n",
    "#         row= [data['title'] ,data['price'], url]\n",
    "        \n",
    "#         writer.writerow(row)\n",
    "#         print(row)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchterm = 'celular'\n",
    "\n",
    "# function to get data\n",
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('server responded:', response.status_code)\n",
    "    else:    \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# function to get several details data\n",
    "def get_detail_data(soup):\n",
    "    try:\n",
    "        title = soup.find('h1', class_='ui-pdp-title').text\n",
    "    except:\n",
    "        title = ''\n",
    "    try:\n",
    "        price = soup.find('div', class_='andes-tooltip__trigger').find('span', class_='price-tag-fraction').text\n",
    "    except:            \n",
    "        price = ''\n",
    "    data = {\n",
    "        'title': title,\n",
    "        'price': price\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def get_index_data(soup):\n",
    "    try:\n",
    "        links = soup.find_all('a', class_='ui-search-item__group__element ui-search-link')\n",
    "    except:\n",
    "        links = []\n",
    "    \n",
    "    urls = [item.get('href') for item in links]\n",
    "    print(len(urls))    \n",
    "    return urls   \n",
    "\n",
    "\n",
    "    \n",
    "def main():\n",
    "    url = 'https://lista.mercadolivre.com.br/celular#D[A:{searchterm}]'\n",
    "    \n",
    "    products = get_index_data(get_page(url))\n",
    "    \n",
    "    for link in products:\n",
    "        data = get_detail_data(get_page(link))\n",
    "        print(data)\n",
    "# function to save data    \n",
    "# def write_csv(data):\n",
    "#     with open('output.csv', 'a') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "        \n",
    "#         writer.writerow()\n",
    "\n",
    "\n",
    "\n",
    "# def parse(soup):\n",
    "#     result = soup.find_all('div', {'class':'ui-search-result__content-wrapper'})\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://lista.mercadolivre.com.br/celular#D[A:celular]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocos = soup.find_all('div', class_='ui-search-result__content-wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"D:/chromedriver2/chromedriver.exe\"\n",
    "\n",
    "option = Options()\n",
    "option.headless=True\n",
    "\n",
    "driver = webdriver.Chrome(PATH, options=option)\n",
    "driver.get(\"https://lista.mercadolivre.com.br/celular#D[A:celular]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element_by_link_text(\"Seguinte\")\n",
    "print(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver\n",
    "\n",
    "for bloco in blocos:\n",
    "    name = bloco.find('h2').text\n",
    "    print(name)\n",
    "    \n",
    "    valor = bloco.find('span', class_='price-tag-fraction').text\n",
    "    print(valor)\n",
    "    print('=======================')\n",
    "\n",
    "print(len(name))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
